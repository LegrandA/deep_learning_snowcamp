{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification des images : Lasagnes vs. Hot-Dog (vs. Hamburgers vs. Raviolis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce notebook, nous allons nous intéresser à la reconnaissance d'images, en bon anglais : **Computer Vision**\n",
    "\n",
    "Nous aborderons les éléments suivants : \n",
    "* Les réseaux de convolutions (a.k.a. convnets a.k.a. CNN)\n",
    "* L'augmentation des données \n",
    "* La réutilisation et l'adaptation de réseaux existants\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le MNIST (le Hello World de la classification d'images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le MNIST (Mixed National Institute of Standards and Technology) est une base de données contenant 70 000 (60 000 pour l'entrainement et 10 000 pour la validation) chiffres écrits à la main. Avec ce jeu de données, nous pouvons construire des algorithmes permettant de reconnaitre les chiffres.\n",
    "\n",
    "Un réseau de neurones traditionnel (totalement connecté), comme pour le précédent exercice de classification du vin, fonctionnera dans une certaine mesure. Cependant, les performances (qualité du modèle évaluée avec la précision des résultats) seront bien meilleures avec un réseau de convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU or not GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction du réseau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre réseau prendra en paramètres des images de chiffres écrits manuellement, ces images ayant pour dimension 28 pixels sur 28, et un seul canal colorimétrique (une image en niveau de gris pour le reformuler clairement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création du modèle et ajout des 3 premières couches (Convolution) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajout des couches totalement connectées (Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Téléchargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affichages des 9 premières images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(train_images[i], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Class {}\".format(train_labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation et redimensionnement des données d'entrée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training, we’ll preprocess the data by reshaping it into the shape the network\n",
    "expects and scaling it so that all values are in the [0, 1] interval. Previously, our train-\n",
    "ing images, for instance, were stored in an array of shape (60000, 28, 28) of type\n",
    "uint8 with values in the [0, 255] interval. We transform it into a float32 array of\n",
    "shape (60000, 28 * 28) with values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = train_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "model.compile(optimizer='rmsprop',\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasagnes vs Hots-Dogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier des chiffres manuscrits est un passage obligé lorsque l'on débute dans la classification d'images. Mais ce type d'architecture peut être utilisé pour classifiers tout type d'images.\n",
    "\n",
    "Ici, nous allons classifier des photos de plats, issus du dataset food101 ( https://www.vision.ee.ethz.ch/datasets_extra/food-101/ )\n",
    "\n",
    "Dans ce dataset nous avons 100 catégories de plats. Chaque catégorie contenant 1000 images.\n",
    "1000 images c'est à la fois beaucoup (il a fallu récupérer ces photographies, les classifier manuellement, etc.) mais c'est très peu pour un algorithme de reconnaissance d'images qui a besoin de beaucoup plus d'images.\n",
    "\n",
    "Cette exercice reprend l'article du blog Keras 'Cats vs. Dogs' (https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html). L'article d'origine permet de classifier entre 2 catégories (classification binaire). Notre objectif lors de cet atelier est de classifier parmi 4 catégories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réutilisation d'un réseau existant\n",
    "\n",
    "Nous pourrions créer notre propre réseau de convolution et l'entrainer avec les 100 000 images du dataset pour calculer les différents paramètres du modèle. Malheureusement, cela serait un peu long (vous pouvez essayer chez vous...).\n",
    "\n",
    "Une autre approche serait de se baser sur un réseau existant. Keras propose différents réseaux pré-entrainés et prêt à l'emploi ( https://keras.io/applications/ ): \n",
    "\n",
    "* VGG16\n",
    "* VGG19\n",
    "* ResNet50\n",
    "* Inception V3\n",
    "* Xception\n",
    "* ..\n",
    "\n",
    "Ces réseaux ont été entrainés sur le dataset ImageNet : 1.4 millions d'images classifiées en 1000 catégories. Outre le fait que ces réseaux fonctionnent déjà très bien pour la reconnaissance d'images, le fait de les avoir entrainés sur un tel volume d'images leur a permis de comprendre de quoi était composé une image : formes, contours, plans, etc.\n",
    "\n",
    "Lorsque nous réutilisons (en les adaptant) ces réseaux pré-entrainés, nous bénéficions de cette connaissance ce qui induit un gain de temps non-négligeable.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "model = VGG16(weights='imagenet', include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(photo):\n",
    "    \"\"\"\n",
    "        Analyse l'image et recherche les catégories auxquelles elle appartient\n",
    "    \"\"\"\n",
    "    img = image.load_img(photo, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    features = model.predict(x)\n",
    "    preds = decode_predictions(features)\n",
    "    return preds\n",
    "\n",
    "def display_photo(photo):\n",
    "    display(Image(filename=photo))\n",
    "    \n",
    "def display_and_predict(photo):\n",
    "    display_photo(photo)\n",
    "    print(\"Predictions = \" + str(predict(photo)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une lasagne..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_and_predict('./snowcamp/images/train/lasagna/1089702.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_and_predict('snowcamp/images/train/lasagna/2009224.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_and_predict('snowcamp/images/train/hot_dog/1000288.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_and_predict('snowcamp/images/train/hot_dog/302949.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_dir = 'snowcamp/images/train/'\n",
    "validation_data_dir = 'snowcamp/images/validation/'\n",
    "img_width, img_height = 224,224\n",
    "nb_categories = 2\n",
    "batch_size = 50\n",
    "nb_train_samples = nb_categories * 800\n",
    "nb_validation_samples = nb_categories * 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation\n",
    "\n",
    "Overfitting is caused by having too few samples to learn from, rendering you unable to train a model that can generalize to new data. Given infinite data, your model would be exposed to every possible aspect of the data distribution at hand: you would never overfit. Data augmentation takes the approach of generating more training data\n",
    "from existing training samples, by augmenting the samples via a number of random\n",
    "transformations that yield believable-looking images. The goal is that at training time,\n",
    "your model will never see the exact same picture twice. This helps expose the model\n",
    "to more aspects of the data and generalize better.\n",
    "In Keras, this can be done by configuring a number of random transformations to\n",
    "be performed on the images read by the ImageDataGenerator instance. Let’s get\n",
    "started with an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just a few of the options available (for more, see the Keras documentation).\n",
    "Let’s quickly go over this code:\n",
    "* rotation_range is a value in degrees (0–180), a range within which to randomly rotate pictures.\n",
    "* width_shift and height_shift are ranges (as a fraction of total width or\n",
    "height) within which to randomly translate pictures vertically or horizontally.\n",
    "* shear_range is for randomly applying shearing transformations.\n",
    "* zoom_range is for randomly zooming inside pictures.\n",
    "* horizontal_flip is for randomly flipping half the images horizontally—rele-\n",
    "vant when there are no assumptions of horizontal asymmetry (for example,\n",
    "real-world pictures).\n",
    "* fill_mode is the strategy used for filling in newly created pixels, which can\n",
    "appear after a rotation or a width/height shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "from keras.preprocessing import image\n",
    "fnames = [os.path.join(train_data_dir + 'lasagna/', fname) for\n",
    "fname in os.listdir(train_data_dir + 'lasagna/')]\n",
    "img_path = fnames[3]\n",
    "\n",
    "img = image.load_img(img_path, target_size=(150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = image.img_to_array(img)\n",
    "x = x.reshape((1,) + x.shape)\n",
    "\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1):\n",
    "    plt.figure(i)\n",
    "    imgplot = plt.imshow(image.array_to_img(batch[0]))\n",
    "    i += 1\n",
    "    if i % 4 == 0:\n",
    "        break\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_bottlebeck_features():\n",
    "    datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "    # build the VGG16 network\n",
    "    model = VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "    print (\"Create train matrix...\")\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    bottleneck_features_train = model.predict_generator(generator, nb_train_samples // batch_size)\n",
    "    print (\"Bottleneck features are OK...\")\n",
    "    np.save(open('bottleneck_features_train_all_v2.npy', 'wb'), bottleneck_features_train)\n",
    "\n",
    "    print (\"Create validation matrix...\")\n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    bottleneck_features_validation = model.predict_generator(\n",
    "        generator, nb_validation_samples // batch_size)\n",
    "    np.save(open('bottleneck_features_validation_all_v2.npy', 'wb'),\n",
    "            bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_bottlebeck_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "\n",
    "epochs=50\n",
    "top_model_weights_path='bottleneck_fc_model_all_v2.h5'\n",
    "\n",
    "#Create labels\n",
    "def create_target_row(nb_categories, cat):\n",
    "    res = [0] * nb_categories\n",
    "    res[cat] = 1\n",
    "    return res\n",
    "\n",
    "def create_target(rows_by_cat, nb_cat):\n",
    "    target = []\n",
    "    for i in range(0,nb_cat):\n",
    "        for j in range(0, rows_by_cat):\n",
    "            target.append(create_target_row(nb_cat, i))\n",
    "    return np.array(target)\n",
    "\n",
    "def train_top_model():\n",
    "    train_data = np.load(open('bottleneck_features_train_all_v2.npy', 'rb'))\n",
    "    # the features were saved in order, so recreating the labels is easy\n",
    "    train_labels = create_target(800, nb_categories)\n",
    "\n",
    "    validation_data = np.load(open('bottleneck_features_validation_all_v2.npy', 'rb'))\n",
    "    validation_labels = create_target(200, nb_categories)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))    \n",
    "    model.add(Dense(nb_categories, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_data, train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels))\n",
    "    model.save_weights(top_model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_top_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "train_data = np.load(open('bottleneck_features_train_all_v2.npy', 'rb'))\n",
    "\n",
    "input_tensor = Input( shape=(img_width,img_height ,3) )\n",
    "base_model = VGG16(weights='imagenet',include_top= False,input_tensor=input_tensor)\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(nb_categories, activation='softmax'))\n",
    "top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "loaded_model = Model(inputs= base_model.input, outputs= top_model(base_model.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = image.load_img('./snowcamp/images/validation/lasagna/3355991.jpg', target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "features = loaded_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = image.load_img('./snowcamp/images/validation/hot_dog/2889560.jpg', target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "features = loaded_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
