{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification des images : Lasagnes vs. Hot-Dog (vs. Hamburgers vs. Raviolis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce notebook, nous allons nous intéresser à la reconnaissance d'images, en bon anglais : **Computer Vision**\n",
    "\n",
    "Nous aborderons les éléments suivants : \n",
    "* Les réseaux de convolutions (a.k.a. convnets a.k.a. CNN)\n",
    "* L'augmentation des données \n",
    "* La réutilisation et l'adaptation de réseaux existants\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le MNIST (le Hello World de la classification d'images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le MNIST (Mixed National Institute of Standards and Technology) est une base de données contenant 70 000 (60 000 pour l'entrainement et 10 000 pour la validation) chiffres écrits à la main. Avec ce jeu de données, nous pouvons construire des algorithmes permettant de reconnaitre les chiffres.\n",
    "\n",
    "Un réseau de neurones traditionnel (totalement connecté), comme pour le précédent exercice de classification du vin, fonctionnera dans une certaine mesure. Cependant, les performances (qualité du modèle évaluée avec la précision des résultats) seront bien meilleures avec un réseau de convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU or not GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction du réseau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre réseau prendra en paramètres des images de chiffres écrits manuellement, ces images ayant pour dimension 28 pixels sur 28, et un seul canal colorimétrique (une image en niveau de gris pour le reformuler clairement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création du modèle et ajout des 3 premières couches (Convolution) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajout des couches totalement connectées (Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "\"\"\"\n",
    "    Ajout d'une couche dense (64 neurones) avec une activation relu\n",
    "\"\"\"\n",
    "model.add(...)\n",
    "\"\"\"\n",
    "    Ajout d'une couche dense (10 neurones car 10 catégories) avec une activation (à choisir ;) )\n",
    "\"\"\"\n",
    "model.add(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Téléchargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affichages des 9 premières images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(train_images[i], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Class {}\".format(train_labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation et redimensionnement des données d'entrée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de procéder à l'entrainement, nous redimensionnons les données pour qu'elles soient compatibles avec le format d'entrée de notre réseau (qui contient 4 dimensions).\n",
    "\n",
    "Before training, we’ll preprocess the data by reshaping it into the shape the network\n",
    "expects and scaling it so that all values are in the [0, 1] interval. Previously, our train-\n",
    "ing images, for instance, were stored in an array of shape (60000, 28, 28) of type\n",
    "uint8 with values in the [0, 255] interval. We transform it into a float32 array of\n",
    "shape (60000, 28 * 28) with values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Nous ajoutons 1 dimension pour 1 canal (le canal des niveaux de gris)\n",
    "\"\"\"\n",
    "train_images = train_images.reshape( (60000, ... , ... , ...) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Nous divisons ensuite les valeurs de \"pixels\" de l'images pour qu'elles soient comprises entre 0 et 1. (au lieu de 0 à 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.astype('float32') / ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Nous effectuons la même manipulation pour les données de test\n",
    "\"\"\"\n",
    "test_images = test_images.reshape((10000, ... , ... , ...))\n",
    "test_images = test_images.astype('float32') / ...\n",
    "\n",
    "\"\"\"\n",
    "    Nous transformons les données cibles en catégories grâce à la fonction to_categorical fournie par keras \n",
    "\"\"\"\n",
    "train_labels = ...\n",
    "test_labels = ...\n",
    "\n",
    "\"\"\"\n",
    "    Nous pouvons  compiler le modèle\n",
    "\"\"\"\n",
    "\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss=...,\n",
    "    metrics=[...])\n",
    "\n",
    "\"\"\"\n",
    "    Nous procédons enfin à l'entrainement du modèle, avec 5 epochs et des batchs de taille 64\n",
    "\"\"\"\n",
    "\n",
    "model.fit(..., ..., ..., ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasagnes vs Hots-Dogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier des chiffres manuscrits est un passage obligé lorsque l'on débute dans la classification d'images. Mais ce type d'architecture peut être utilisé pour classifiers tout type d'images.\n",
    "\n",
    "Ici, nous allons classifier des photos de plats, issus du dataset food101 ( https://www.vision.ee.ethz.ch/datasets_extra/food-101/ )\n",
    "\n",
    "Dans ce dataset nous avons 100 catégories de plats. Chaque catégorie contenant 1000 images.\n",
    "1000 images c'est à la fois beaucoup (il a fallu récupérer ces photographies, les classifier manuellement, etc.) mais c'est très peu pour un algorithme de reconnaissance d'images qui a besoin de beaucoup plus d'images.\n",
    "\n",
    "Cette exercice reprend l'article du blog Keras 'Cats vs. Dogs' (https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html). L'article d'origine permet de classifier entre 2 catégories (classification binaire). Notre objectif lors de cet atelier est de classifier parmi 4 catégories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réutilisation d'un réseau existant\n",
    "\n",
    "Nous pourrions créer notre propre réseau de convolution et l'entrainer avec les 100 000 images du dataset pour calculer les différents paramètres du modèle. Malheureusement, cela serait un peu long (vous pouvez essayer chez vous...).\n",
    "\n",
    "Une autre approche serait de se baser sur un réseau existant. Keras propose différents réseaux pré-entrainés et prêt à l'emploi ( https://keras.io/applications/ ): \n",
    "\n",
    "* VGG16\n",
    "* VGG19\n",
    "* ResNet50\n",
    "* Inception V3\n",
    "* Xception\n",
    "* ..\n",
    "\n",
    "Ces réseaux ont été entrainés sur le dataset ImageNet : 1.4 millions d'images classifiées en 1000 catégories. Outre le fait que ces réseaux fonctionnent déjà très bien pour la reconnaissance d'images, le fait de les avoir entrainés sur un tel volume d'images leur a permis de comprendre de quoi était composé une image : formes, contours, plans, etc.\n",
    "\n",
    "Lorsque nous réutilisons (en les adaptant) ces réseaux pré-entrainés, nous bénéficions de cette connaissance ce qui induit un gain de temps non-négligeable.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "model = VGG16(weights='imagenet', include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(photo):\n",
    "    \"\"\"\n",
    "        Analyse l'image et recherche les catégories auxquelles elle appartient\n",
    "    \"\"\"\n",
    "    img = image.load_img(photo, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    features = model.predict(x)\n",
    "    preds = decode_predictions(features)\n",
    "    return preds\n",
    "\n",
    "def display_photo(photo):\n",
    "    display(Image(filename=photo))\n",
    "    \n",
    "def display_and_predict(photo):\n",
    "    display_photo(photo)\n",
    "    print(\"Predictions = \" + str(predict(photo)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une lasagne..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_and_predict('datasets/images/train/lasagna/1089702.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_and_predict('datasets/images/train/lasagna/2009224.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_and_predict('datasets/images/train/hot_dog/1000288.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_and_predict('datasets/images/train/hot_dog/302949.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réutilisation du réseau pré-existant pour ajouter nos propres catégories (Feature extraction)\n",
    "\n",
    "Le réseau existant catégorise donc déjà nos images. Cependant, il y a beaucoup de catégories possibles et elles ne correspondent pas forcément à notre classification (hotdog, ravioli, lasagne, hamburder).\n",
    "\n",
    "Pour ajouter nos propres catégories, nous pouvons le faire en 3 temps:\n",
    "\n",
    "* Ajout de notre propre **classifier** qui remplace le classifier fourni avec VGG16.\n",
    "* **Extension** du modèle (blocs de convolution), en ajoutant un réseau de neurones au dessus des hautes, et en réexécutant l'ensemble du modèle sur les images.\n",
    "* Recalcul (**fine tuning**) des poids des derniers blocs de convolution (fine tuning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'datasets/images/train/'\n",
    "validation_data_dir = 'datasets/images/validation/'\n",
    "\"\"\"\n",
    "    Les largeurs et hauteurs d'images devraient être 224\n",
    "\"\"\"\n",
    "img_width, img_height = ... , ...\n",
    "\n",
    "\"\"\"\n",
    "    Nombre de catégories (2 mini). Il faudra adapter les contenus de <train_data_dir> et <validation_data_dir> en fonction\n",
    "\"\"\"\n",
    "nb_categories = 2 #ou 4 pour les joueurs\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "\"\"\"\n",
    "    800 images / catégorie pour l'entrainement\n",
    "\"\"\"\n",
    "nb_train_samples = nb_categories * 800\n",
    "\n",
    "\"\"\"\n",
    "    200 images / catégorie pour la validation\n",
    "\"\"\"\n",
    "nb_validation_samples = nb_categories * 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajout de notre classifier\n",
    "\n",
    "Cette première étape permet d'avoir de bons résultats et de bonnes performances, y compris avec un CPU.\n",
    "\n",
    "Afin d'entrainer le réseau, vous devez au préalable séparer les images fournies en 2 parties:\n",
    "* 800 images (par catégorie) restent dans les répertoires d'origine.\n",
    "* 200 images (par catégorie) doivent se retrouver dan le répertoire **validation**.\n",
    "\n",
    "Votre hiérarchie de répertoire devrait être identique à celle ci:\n",
    "\n",
    "\n",
    "\n",
    "| ├── train\n",
    "| │   ├── hamburger (800 images)\n",
    "| │   ├── hot_dog (800 images)\n",
    "| │   ├── lasagna (800 images)\n",
    "| │   └── ravioli (800 images)\n",
    "| └── validation\n",
    "|    ├── hamburger (200 images)\n",
    "|    ├── hot_dog (200 images)\n",
    "|    ├── lasagna (200 images)\n",
    "|    └── ravioli (200 images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    On instancie un modèle basé sur VGG16 (sans le top....)\n",
    "\"\"\"\n",
    "\n",
    "def save_bottleneck_features():\n",
    "    # build the VGG16 network\n",
    "    model = VGG16(include_top=..., weights='imagenet')\n",
    "\n",
    "    print (\"Create train matrix...\")\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    bottleneck_features_train = model.predict_generator(generator, nb_train_samples // batch_size)\n",
    "    print (\"Bottleneck features are OK...\")\n",
    "    np.save(open('bottleneck_features_train_all_v2.npy', 'wb'), bottleneck_features_train)\n",
    "\n",
    "    print (\"Create validation matrix...\")\n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    bottleneck_features_validation = model.predict_generator(\n",
    "        generator, nb_validation_samples // batch_size)\n",
    "    np.save(open('bottleneck_features_validation_all_v2.npy', 'wb'),\n",
    "            bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_bottleneck_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "\"\"\"\n",
    "    Nombre d'epochs (50 pour commencer)\n",
    "\"\"\"\n",
    "epochs = ...\n",
    "top_model_weights_path='bottleneck_fc_model_all_v2.h5'\n",
    "\n",
    "#Create labels\n",
    "def create_target_row(nb_categories, cat):\n",
    "    res = [0] * nb_categories\n",
    "    res[cat] = 1\n",
    "    return res\n",
    "\n",
    "def create_target(rows_by_cat, nb_cat):\n",
    "    target = []\n",
    "    for i in range(0,nb_cat):\n",
    "        for j in range(0, rows_by_cat):\n",
    "            target.append(create_target_row(nb_cat, i))\n",
    "    return np.array(target)\n",
    "\n",
    "def train_top_model():\n",
    "    train_data = np.load(open('bottleneck_features_train_all_v2.npy', 'rb'))\n",
    "    # the features were saved in order, so recreating the labels is easy\n",
    "    \"\"\"\n",
    "        On crée autant de lignes que d'images 'train'\n",
    "    \"\"\"\n",
    "    train_labels = create_target( ... , nb_categories)\n",
    "\n",
    "    validation_data = np.load(open('bottleneck_features_validation_all_v2.npy', 'rb'))\n",
    "    \n",
    "    \"\"\"\n",
    "        On crée autant de lignes que d'images 'test'\n",
    "    \"\"\"\n",
    "    validation_labels = create_target( ... , nb_categories)\n",
    "\n",
    "    \"\"\"\n",
    "        Ajout du classifier\n",
    "        La première couche dense aura 256 neurones et une activation relu\n",
    "        Le dropout aura pour valeur entre 0 et 0.5 (voire plus pour les joueurs ;) )\n",
    "        Et enfin la dernière couche servira à déterminer la classe résultante\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(... , activation= ...))\n",
    "    model.add(Dropout(... ))\n",
    "    model.add(Dense(..., activation = ...))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(train_data, train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels))\n",
    "    \n",
    "    model.save_weights(top_model_weights_path)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_top_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quelques prédictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "\"\"\"\n",
    "    On ouvre la matrice des bottleneck features (train)\n",
    "\"\"\"\n",
    "train_data = np.load(open('bottleneck_features_train_all_v2.npy', 'rb'))\n",
    "\n",
    "input_tensor = Input( shape=(img_width,img_height ,3) )\n",
    "base_model = VGG16(weights='imagenet',include_top= False,input_tensor=input_tensor)\n",
    "\n",
    "\"\"\"\n",
    "    On redéfinit le classifier\n",
    "\"\"\"\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "top_model.add(Dense(..., activation=...))\n",
    "top_model.add(Dropout(0.5))    \n",
    "top_model.add(Dense(nb_categories, activation='softmax'))\n",
    "\n",
    "top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "\"\"\"\n",
    "    On charge le modèle\n",
    "\"\"\"\n",
    "\n",
    "loaded_model = Model(inputs= base_model.input, outputs= top_model(base_model.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.load_img('datasets/images/validation/lasagna/3355991.jpg', target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "features = loaded_model.predict(x)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.load_img('datasets/images/validation/hot_dog/2889560.jpg', target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "features = loaded_model.predict(x)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction + Data augmentation\n",
    "\n",
    "Avec peu d'images, il est peu probable qu'un réseau de convolution généralise correctement. Il ne saura pas catégoriser de nouvelles images (jamais vues) alors qu'il sera très bon sur les images utilisées lors de l'entrainement (Overfitting).\n",
    "\n",
    "Pour éviter ce phénomène, nous allons générer de nouvelles images issues de celles en notre possession et auxquelles nous  avons appliquer un certain nombre de transformations aléatoires. \n",
    "\n",
    "La classe ImageDataGenerator permet d'effectuer tout cela avec Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just a few of the options available (for more, see the Keras documentation).\n",
    "Let’s quickly go over this code:\n",
    "* rotation_range is a value in degrees (0–180), a range within which to randomly rotate pictures.\n",
    "* width_shift and height_shift are ranges (as a fraction of total width or\n",
    "height) within which to randomly translate pictures vertically or horizontally.\n",
    "* shear_range is for randomly applying shearing transformations.\n",
    "* zoom_range is for randomly zooming inside pictures.\n",
    "* horizontal_flip is for randomly flipping half the images horizontally—rele-\n",
    "vant when there are no assumptions of horizontal asymmetry (for example,\n",
    "real-world pictures).\n",
    "* fill_mode is the strategy used for filling in newly created pixels, which can\n",
    "appear after a rotation or a width/height shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Initalisez le  ImageDataGenerator en positionnant les attributs:\n",
    "    *  rotation_range\n",
    "    *  width_shift_range\n",
    "    * height_shift_range\n",
    "    * shear_range\n",
    "    * zoom_range\n",
    "\"\"\"\n",
    "datagen = ImageDataGenerator(\n",
    "    ...,\n",
    "    ...,\n",
    "    ...,\n",
    "    ...,\n",
    "    ...,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Sélection d'une image sur laquelle nous allons appliquer nos transformations aléatoires\n",
    "\"\"\"\n",
    "import os \n",
    "\n",
    "from keras.preprocessing import image\n",
    "fnames = [os.path.join(train_data_dir + 'lasagna/', fname) for\n",
    "fname in os.listdir(train_data_dir + 'lasagna/')]\n",
    "img_path = fnames[3]\n",
    "\n",
    "img = image.load_img(img_path, target_size=(150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Visualisation des transformations\n",
    "\"\"\"\n",
    "x = image.img_to_array(img)\n",
    "x = x.reshape((1,) + x.shape)\n",
    "\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1):\n",
    "    plt.figure(i)\n",
    "    imgplot = plt.imshow(image.array_to_img(batch[0]))\n",
    "    i += 1\n",
    "    if i % 4 == 0:\n",
    "        break\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "conv_base = VGG16(weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(150, 150, 3))\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(nb_categories, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    class_mode='categorical')\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "metrics=['acc'])\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
